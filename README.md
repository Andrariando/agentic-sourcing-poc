# Agentic Sourcing Copilot â€” Enterprise POC

A **human-in-the-loop, multi-agent decision-support system** for procurement sourcing, built on the Dynamic Transaction Pipeline (DTP-01 to DTP-06) methodology.

> **Phase 3** â€” Enterprise Memory & Evidence Implementation

---

## ğŸ¯ Design Philosophy

| Principle | Implementation |
|-----------|----------------|
| **Decision is focal point** | AI advises, human decides |
| **Rules > LLM** | Deterministic rules before any LLM reasoning |
| **No autonomous decisions** | All recommendations require human approval |
| **Full traceability** | Every output attributable to inputs, rules, data, and agent |
| **Grounded retrieval** | Answers cite uploaded documents/data |

---

## ğŸ—ï¸ Architecture

### Folder Structure

```
agentic-sourcing-poc/
â”‚
â”œâ”€â”€ frontend/                    # Streamlit UI only
â”‚   â”œâ”€â”€ app.py                   # Main entry point
â”‚   â”œâ”€â”€ api_client.py            # Backend communication
â”‚   â””â”€â”€ pages/
â”‚       â”œâ”€â”€ case_dashboard.py    # Case list & metrics
â”‚       â”œâ”€â”€ case_copilot.py      # Decision console + chat
â”‚       â””â”€â”€ knowledge_management.py  # Document/data upload
â”‚
â”œâ”€â”€ backend/                     # All business logic
â”‚   â”œâ”€â”€ main.py                  # FastAPI server (API entry point)
â”‚   â”‚
â”‚   â”œâ”€â”€ supervisor/              # Central orchestration
â”‚   â”‚   â”œâ”€â”€ graph.py             # LangGraph workflow
â”‚   â”‚   â”œâ”€â”€ state.py             # State management
â”‚   â”‚   â””â”€â”€ router.py            # Intent classification
â”‚   â”‚
â”‚   â”œâ”€â”€ agents/                  # Specialized agents
â”‚   â”‚   â”œâ”€â”€ base.py              # Base agent with retrieval tools
â”‚   â”‚   â”œâ”€â”€ strategy.py          # DTP-01 Strategy Agent
â”‚   â”‚   â”œâ”€â”€ supplier_eval.py     # DTP-03/04 Supplier Agent
â”‚   â”‚   â”œâ”€â”€ negotiation.py       # DTP-04 Negotiation Agent
â”‚   â”‚   â””â”€â”€ signal.py            # Signal Interpretation Agent
â”‚   â”‚
â”‚   â”œâ”€â”€ ingestion/               # Data ingestion pipelines
â”‚   â”‚   â”œâ”€â”€ document_ingest.py   # PDF/DOCX/TXT â†’ ChromaDB (RAG)
â”‚   â”‚   â”œâ”€â”€ data_ingest.py       # CSV/Excel â†’ SQLite (Data Lake)
â”‚   â”‚   â””â”€â”€ validators.py        # Schema validation
â”‚   â”‚
â”‚   â”œâ”€â”€ rag/                     # Vector retrieval
â”‚   â”‚   â”œâ”€â”€ vector_store.py      # ChromaDB wrapper
â”‚   â”‚   â””â”€â”€ retriever.py         # Document retriever
â”‚   â”‚
â”‚   â”œâ”€â”€ persistence/             # Data lake
â”‚   â”‚   â”œâ”€â”€ database.py          # SQLite connection
â”‚   â”‚   â””â”€â”€ models.py            # SQLModel tables
â”‚   â”‚
â”‚   â””â”€â”€ services/                # Business logic layer
â”‚       â”œâ”€â”€ case_service.py      # Case management
â”‚       â”œâ”€â”€ chat_service.py      # Copilot with Supervisor
â”‚       â””â”€â”€ ingestion_service.py # Ingestion orchestration
â”‚
â”œâ”€â”€ shared/                      # Cross-cutting modules
â”‚   â”œâ”€â”€ schemas.py               # Pydantic schemas
â”‚   â””â”€â”€ constants.py             # Enums & constants
â”‚
â”œâ”€â”€ data/                        # Synthetic data & databases
â”‚   â”œâ”€â”€ datalake.db              # SQLite data lake
â”‚   â””â”€â”€ chroma_db/               # ChromaDB vector store
â”‚
â””â”€â”€ requirements.txt
```

### System Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         FRONTEND (Streamlit)                        â”‚
â”‚  Case Dashboard â”‚ Case Copilot (Decision Console) â”‚ Knowledge Mgmt  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼ API Client
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         BACKEND (FastAPI)                           â”‚
â”‚  /api/cases â”‚ /api/chat â”‚ /api/decisions â”‚ /api/ingest â”‚ /health    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â–¼                    â–¼                    â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚  SUPERVISOR â”‚      â”‚    RAG      â”‚      â”‚  DATA LAKE  â”‚
      â”‚  (LangGraph)â”‚      â”‚ (ChromaDB)  â”‚      â”‚  (SQLite)   â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â–¼         â–¼         â–¼         â–¼         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Strategyâ”‚â”‚Supplierâ”‚â”‚Negotia-â”‚â”‚Contractâ”‚â”‚ Signal â”‚
â”‚ Agent  â”‚â”‚ Agent  â”‚â”‚tion    â”‚â”‚ Agent  â”‚â”‚ Agent  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quick Start

### Prerequisites
- Python 3.9+
- OpenAI API key

### Installation

```bash
# Clone repository
git clone https://github.com/Andrariando/agentic-sourcing-poc.git
cd agentic-sourcing-poc

# Install dependencies
pip install -r requirements.txt

# Create .env file
echo "OPENAI_API_KEY=your-key-here" > .env
```

### Running Locally

#### Option 1: Separated Mode (Full Architecture)

```bash
# Terminal 1: Start Backend
python -m uvicorn backend.main:app --reload --port 8000

# Terminal 2: Start Frontend
streamlit run frontend/app.py
```

- **API**: http://localhost:8000
- **API Docs**: http://localhost:8000/docs
- **Frontend**: http://localhost:8501

#### Option 2: Integrated Mode (Single Process)

```bash
# Set environment variable
export USE_INTEGRATED_MODE=true

# Run frontend only (backend runs in-process)
streamlit run frontend/app.py
```

---

## ğŸ“¡ API Reference

### Case Management
| Method | Endpoint | Description |
|--------|----------|-------------|
| GET | `/api/cases` | List all cases |
| GET | `/api/cases/{id}` | Get case details |
| POST | `/api/cases` | Create new case |

### Copilot Chat
| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/api/chat` | Send message to copilot |

### Human Decisions
| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/api/decisions/approve` | Approve recommendation |
| POST | `/api/decisions/reject` | Reject recommendation |

### Document Ingestion (RAG)
| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/api/ingest/document` | Upload PDF/DOCX/TXT |
| GET | `/api/documents` | List ingested documents |
| DELETE | `/api/documents/{id}` | Delete document |

### Structured Data Ingestion
| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/api/ingest/data/preview` | Preview CSV/Excel |
| POST | `/api/ingest/data` | Ingest to data lake |
| GET | `/api/ingest/history` | View ingestion history |

### System
| Method | Endpoint | Description |
|--------|----------|-------------|
| GET | `/health` | Health check |

---

## ğŸ” Governance Rules

1. **Frontend never calls agents directly** â€” All via API â†’ Supervisor
2. **Supervisor is single source of truth** â€” State managed centrally
3. **Intent classification is deterministic** â€” Keyword-based, no LLM
4. **Exploration doesn't change state** â€” EXPLORE intent is read-only
5. **Stage advancement requires approval** â€” Human clicks Approve/Reject
6. **Retrieval is stage-gated** â€” Documents filtered by DTP relevance

---

## ğŸ¨ UI Design

Enterprise-grade decision console using MIT color system:

| Color | Hex | Usage |
|-------|-----|-------|
| MIT Navy | `#003A8F` | Headers, structure |
| MIT Cardinal Red | `#A31F34` | Actions, alerts |
| Near Black | `#1F1F1F` | Body text |
| Charcoal | `#4A4A4A` | Secondary text |
| Light Gray | `#D9D9D9` | Borders |
| White | `#FFFFFF` | Backgrounds |

### Page Layout

**Case Copilot** â€” 3-column decision console:
- **Left**: Case Context (read-only)
- **Center**: Decision Card + Evidence + Approval Buttons
- **Right**: Copilot Chat (fixed-height, scrollable)

---

## ğŸ§ª Demo Scenarios

### 1. Upload Contract â†’ Ask Grounded Question
1. Go to **Knowledge & Data**
2. Upload a contract PDF with metadata
3. Create/open a case
4. Ask: "What are the key terms in the contract?"
5. âœ… Answer cites uploaded document

### 2. Upload KPI Data â†’ Performance Question
1. Upload CSV with supplier performance data
2. Ask: "How is supplier X performing?"
3. âœ… Answer cites data lake

### 3. Stage Gating
1. Open case at DTP-01
2. Ask: "Execute the contract"
3. âœ… System explains action not available at current stage

### 4. Human Approval Flow
1. Ask: "Recommend a strategy"
2. Agent provides recommendation
3. Click **Approve** â†’ Stage advances
4. Click **Reject** â†’ Stays at current stage

---

## ğŸ“ Data

Synthetic test data generated via `backend/seed_data.py`:
- Cases at various DTP stages
- Supplier performance metrics
- Spend data
- SLA events

Seed demo data from the Dashboard â†’ "Seed Demo Cases" button.

---

## ğŸš¢ Deployment

### Streamlit Cloud (Integrated Mode)

1. Push to GitHub
2. Connect repo on [Streamlit Cloud](https://streamlit.io/cloud)
3. Set secrets:
   ```
   OPENAI_API_KEY = "sk-..."
   ```
4. Deploy

### Production (Separated Mode)

Deploy backend on:
- AWS (EC2, ECS, Lambda)
- Google Cloud Run
- Azure App Service
- Render / Railway

Set environment variable:
```bash
API_BASE_URL=https://your-backend-url.com
USE_API_MODE=true
```

---

## âš ï¸ Notes

- **Research POC** â€” Not production-ready
- **Synthetic data** â€” All metrics are illustrative
- **Token limits** â€” 3,000 tokens per case cap
- **No authentication** â€” Add API keys for production

---

## ğŸ“œ License

Research POC â€” Not for production use

---

## ğŸ› ï¸ Development Phases

| Phase | Focus | Status |
|-------|-------|--------|
| Phase 1 | Core agentic workflow, LangGraph integration | âœ… Complete |
| Phase 2 | Collaboration mode, constraint extraction | âœ… Complete |
| Phase 3 | Enterprise memory (RAG + Data Lake), UI refactor | âœ… Complete |
| Phase 4 | Authentication, audit logging, production hardening | ğŸ”œ Planned |
